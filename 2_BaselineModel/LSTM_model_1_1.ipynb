{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the folder containing 15-minute set files\n",
    "data_folder = './time_sets'\n",
    "\n",
    "# Initialize an empty DataFrame to combine all processed sets\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# Loop through all CSV files in the folder\n",
    "for file in os.listdir(data_folder):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Debugging: Show initial data structure\n",
    "        print(f\"Processing file: {file}\")\n",
    "        print(f\"Initial data shape: {df.shape}\")\n",
    "\n",
    "        # Ensure Timestamp is a proper datetime object\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n",
    "        print(\"Data types after Timestamp conversion:\\n\", df.dtypes)\n",
    "\n",
    "        # Drop rows with invalid timestamps\n",
    "        invalid_rows = df[df['Timestamp'].isna()]\n",
    "        if not invalid_rows.empty:\n",
    "            print(\"Rows with invalid timestamps:\\n\", invalid_rows)\n",
    "            df = df.dropna(subset=['Timestamp'])\n",
    "\n",
    "        # Sort by Timestamp\n",
    "        df = df.sort_values('Timestamp')\n",
    "\n",
    "        # Set Timestamp as index\n",
    "        df = df.set_index('Timestamp')\n",
    "        print(\"Index after setting Timestamp:\\n\", df.index)\n",
    "\n",
    "        # Remove duplicate timestamps\n",
    "        if df.index.duplicated().any():\n",
    "            print(\"Duplicate timestamps detected. Removing duplicates.\")\n",
    "            df = df[~df.index.duplicated(keep='first')]\n",
    "\n",
    "        # Debugging: Check time range and unique timestamps\n",
    "        print(\"Time range:\", df.index.min(), \"to\", df.index.max())\n",
    "        print(\"Number of unique timestamps:\", df.index.nunique())\n",
    "\n",
    "        # Resample to 1-minute intervals\n",
    "        df_resampled = df.resample('1T').asfreq()\n",
    "        print(\"Data after resampling, before interpolation:\\n\", df_resampled.head(10))\n",
    "\n",
    "        # Interpolate missing values\n",
    "        df = df_resampled.interpolate(method='linear').reset_index()\n",
    "\n",
    "        # Debugging: Check the final output\n",
    "        print(\"Data shape after interpolation:\", df.shape)\n",
    "        print(\"Head of interpolated data:\\n\", df.head())\n",
    "\n",
    "        # Ensure required columns exist\n",
    "        required_columns = ['Latitude', 'Longitude', 'SOG', 'COG', 'Heading', 'Navigational status']\n",
    "        if all(col in df.columns for col in required_columns):\n",
    "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Skipping {file}, missing required columns.\")\n",
    "\n",
    "# Feature selection\n",
    "features = ['Latitude', 'Longitude', 'SOG', 'COG', 'Heading']\n",
    "target = 'Navigational status'\n",
    "\n",
    "# Drop rows with missing values in the selected columns\n",
    "all_data = all_data.dropna(subset=features + [target])\n",
    "\n",
    "# Encode the target variable (categorical to numeric)\n",
    "all_data[target] = all_data[target].astype('category').cat.codes\n",
    "\n",
    "# Debugging: Inspect class distribution\n",
    "print(\"Class distribution:\\n\", all_data[target].value_counts())\n",
    "\n",
    "# Define feature matrix (X) and target vector (y)\n",
    "X = all_data[features]\n",
    "y = all_data[target]\n",
    "\n",
    "# Handle class imbalance with class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(\"Computed class weights:\\n\", class_weight_dict)\n",
    "\n",
    "# Split the dataset into training and test sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest classifier with class weights\n",
    "model = RandomForestClassifier(random_state=42, class_weight=class_weight_dict)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Confusion Matrix\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
