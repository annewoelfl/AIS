{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def download_and_extract(zip_urls, download_folder, extract_folder):\n",
    "    \"\"\"\n",
    "    Downloads ZIP files from provided URLs and extracts their contents.\n",
    "    \"\"\"\n",
    "    os.makedirs(download_folder, exist_ok=True)\n",
    "    os.makedirs(extract_folder, exist_ok=True)\n",
    "    \n",
    "    for idx, url in enumerate(zip_urls):\n",
    "        print(f\"({idx + 1}/{len(zip_urls)}) Downloading: {url}\")\n",
    "        zip_filename = os.path.join(download_folder, os.path.basename(url))\n",
    "        \n",
    "        # Download the file\n",
    "        with requests.get(url, stream=True) as response:\n",
    "            response.raise_for_status()\n",
    "            with open(zip_filename, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        \n",
    "        print(f\"Downloaded: {zip_filename}\")\n",
    "        \n",
    "        # Extract the ZIP file\n",
    "        with ZipFile(zip_filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_folder)\n",
    "            print(f\"Extracted: {zip_filename} to {extract_folder}\")\n",
    "\n",
    "\n",
    "def process_large_csv(csv_files, temp_folder, chunk_size=100000):\n",
    "    \"\"\"Reads large CSV files and splits them into individual MMSI files.\"\"\"\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "    mmsi_dict = {}\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"Processing {csv_file}...\")\n",
    "        with tqdm(total=sum(1 for _ in open(csv_file)) - 1, desc=\"Processing chunks\", unit=\"rows\") as pbar:\n",
    "            for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "                # Filter out rows with 'Unknown value' in Navigational Status\n",
    "                chunk = chunk[chunk['Navigational status'] != 'Unknown value']\n",
    "                for mmsi, group in chunk.groupby('MMSI'):\n",
    "                    if mmsi not in mmsi_dict:\n",
    "                        mmsi_dict[mmsi] = group\n",
    "                    else:\n",
    "                        mmsi_dict[mmsi] = pd.concat([mmsi_dict[mmsi], group])\n",
    "                pbar.update(len(chunk))\n",
    "    \n",
    "    # Save individual MMSI files\n",
    "    for mmsi, group in tqdm(mmsi_dict.items(), desc=\"Saving MMSI files\"):\n",
    "        output_file = os.path.join(temp_folder, f\"{mmsi}.csv\")\n",
    "        group.to_csv(output_file, index=False)\n",
    "\n",
    "def filter_by_area(input_folder, output_folder, area_bounds):\n",
    "    \"\"\"Filters MMSI files for entries within a specified geographical area.\"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    min_lat, max_lat, min_lon, max_lon = area_bounds\n",
    "\n",
    "    for file in tqdm(os.listdir(input_folder), desc=\"Filtering by area\"):\n",
    "        if not file.endswith('.csv'):\n",
    "            continue\n",
    "        df = pd.read_csv(os.path.join(input_folder, file))\n",
    "        df = df[(df['Latitude'] >= min_lat) & (df['Latitude'] <= max_lat) & \n",
    "                (df['Longitude'] >= min_lon) & (df['Longitude'] <= max_lon)]\n",
    "        if len(df) >= 10:  # Ensure at least 10 consecutive entries\n",
    "            df.to_csv(os.path.join(output_folder, file), index=False)\n",
    "\n",
    "def extract_30min_sets(input_folder, output_folder):\n",
    "    \"\"\"Extracts 30-minute sets with consistent Navigational Status.\"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for file in tqdm(os.listdir(input_folder), desc=\"Extracting 30-min sets\"):\n",
    "        if not file.endswith('.csv'):\n",
    "            continue\n",
    "        df = pd.read_csv(os.path.join(input_folder, file))\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "        df = df.sort_values('Timestamp')\n",
    "        \n",
    "        # Group into 30-minute intervals\n",
    "        df['Time_Group'] = (df['Timestamp'] - df['Timestamp'].min()).dt.total_seconds() // (30 * 60)\n",
    "        grouped = df.groupby('Time_Group')\n",
    "        \n",
    "        valid_sets = []\n",
    "        for _, group in grouped:\n",
    "            if group['Navigational status'].nunique() == 1:\n",
    "                valid_sets.append(group)\n",
    "\n",
    "        if valid_sets:\n",
    "            final_df = pd.concat(valid_sets)\n",
    "            final_df.to_csv(os.path.join(output_folder, file), index=False)\n",
    "\n",
    "def analyze_navigational_status(input_folder):\n",
    "    \"\"\"Analyzes the distribution of 30-minute sets by Navigational Status.\"\"\"\n",
    "    status_counts = {}\n",
    "\n",
    "    for file in tqdm(os.listdir(input_folder), desc=\"Analyzing Navigational Status\"):\n",
    "        if not file.endswith('.csv'):\n",
    "            continue\n",
    "        df = pd.read_csv(os.path.join(input_folder, file))\n",
    "        status = df['Navigational status'].iloc[0]  # Assuming consistent status in each set\n",
    "        status_counts[status] = status_counts.get(status, 0) + 1\n",
    "\n",
    "    return status_counts\n",
    "\n",
    "def integrate_pipeline_with_web_sources(zip_urls, temp_folder, area_bounds):\n",
    "    \"\"\"\n",
    "    Downloads and processes ZIP files, extracting CSVs and running the pipeline.\n",
    "    \"\"\"\n",
    "    download_folder = './downloaded_zips'\n",
    "    extract_folder = './extracted_files'\n",
    "\n",
    "    # Step 1: Download and Extract\n",
    "    download_and_extract(zip_urls, download_folder, extract_folder)\n",
    "\n",
    "    # Step 2: Split CSVs into MMSI-specific files\n",
    "    process_large_csv([os.path.join(extract_folder, f) for f in os.listdir(extract_folder) if f.endswith('.csv')], temp_folder)\n",
    "\n",
    "    # Step 3: Filter files by geographical area\n",
    "    filtered_folder = './filtered_area'\n",
    "    filter_by_area(temp_folder, filtered_folder, area_bounds)\n",
    "\n",
    "    # Step 4: Extract 30-minute sets\n",
    "    sets_folder = './30min_sets'\n",
    "    extract_30min_sets(filtered_folder, sets_folder)\n",
    "\n",
    "    # Step 5: Analyze Navigational Status\n",
    "    status_distribution = analyze_navigational_status(sets_folder)\n",
    "    print(\"Distribution of Navigational Status:\", status_distribution)\n",
    "\n",
    "# Example URLs for ZIP files (replace with actual URLs)\n",
    "zip_urls = [\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-11-30.zip\",\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-11-29.zip\",\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-11-28.zip\"\n",
    "]\n",
    "\n",
    "# Example usage\n",
    "area_bounds = (7.0, 8.0, 55.0, 56.0)  # Define your area\n",
    "temp_folder = './mmsi_temp'\n",
    "integrate_pipeline_with_web_sources(zip_urls, temp_folder, area_bounds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
